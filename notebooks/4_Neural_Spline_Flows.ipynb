{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.flows import Flow\n",
    "\n",
    "class SplineCouplingLayer(Flow):\n",
    "    \"\"\"\n",
    "    Implements a single coupling layer from Real NVP.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dim, hidden_dim, mask):\n",
    "        super(SplineCouplingLayer, self).__init__()\n",
    "        \n",
    "        self.mask = mask\n",
    "\n",
    "        # The conditioner networks for scale (s) and bias (b)\n",
    "        # These should be simple MLPs that take the \"control\" part of the input\n",
    "        # and output the parameters for the other part.\n",
    "        self.s_net = nn.Sequential(\n",
    "            nn.Linear(data_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, data_dim)\n",
    "        )\n",
    "        self.b_net = nn.Sequential(\n",
    "            nn.Linear(data_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, data_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Computes the forward pass x = f(z).\n",
    "        z -> x\n",
    "        \"\"\"\n",
    "        # The mask determines which part is transformed (mask == 1) \n",
    "        # and which part is the identity (mask == 0).\n",
    "        z_a = z * self.mask  # This is z_A in the equations, but with zeros for z_B\n",
    "        \n",
    "        # The conditioner networks only see the identity part\n",
    "        s = self.s_net(z_a)\n",
    "        b = self.b_net(z_a)\n",
    "        \n",
    "        # Apply the transformation to the other part\n",
    "        # z_b is selected by (1 - mask)\n",
    "        x = z_a + (1 - self.mask) * (z * torch.exp(s) + b) #this transformation is different from the coupling layer\n",
    "        \n",
    "        # The log-determinant is the sum of s for the transformed dimensions\n",
    "        log_det_J = ((1 - self.mask) * s).sum(dim=1)\n",
    "        \n",
    "        return x, log_det_J\n",
    "\n",
    "    def inverse(self, x):\n",
    "        \"\"\"\n",
    "        Computes the inverse pass z = g(x).\n",
    "        x -> z\n",
    "        \"\"\"\n",
    "        # The mask determines which part was the identity\n",
    "        x_a = x * self.mask # This is x_A in the equations\n",
    "        \n",
    "        # The conditioner networks see the identity part\n",
    "        s = self.s_net(x_a)\n",
    "        b = self.b_net(x_a)\n",
    "        \n",
    "        # Apply the inverse transformation to the other part\n",
    "        z = x_a + (1 - self.mask) * ((x - b) * torch.exp(-s))\n",
    "        \n",
    "        # The log-determinant of the inverse Jacobian\n",
    "        log_det_J_inv = ((1 - self.mask) * -s).sum(dim=1)\n",
    "\n",
    "        return z, log_det_J_inv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
